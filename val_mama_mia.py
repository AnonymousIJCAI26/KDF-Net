import argparse
import os
import yaml
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from collections import OrderedDict, defaultdict
import datetime
import nibabel as nib

import archs
from mama_mia_dataset import MAMAMIADataset2D, save_prediction_as_nifti
from metrics import iou_score, indicators
from utils import AverageMeter


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--name', required=True, help='model name')
    parser.add_argument('--datasets', nargs='+', required=True, 
                       help='Datasets to test: DUKE, NACT, ISPY1, ISPY2')
    parser.add_argument('--output_dir', default='outputs_mama_mia')
    parser.add_argument('--batch_size', default=8, type=int)
    parser.add_argument('--save_predictions', action='store_true', 
                       help='æ˜¯å¦ä¿å­˜é¢„æµ‹çš„åˆ†å‰²ç»“æœ'),
    parser.add_argument('--threshold', default=0.5, type=float,
                       help='äºŒå€¼åŒ–é˜ˆå€¼ (é»˜è®¤: 0.5)')
    # ã€æ–°å¢ã€‘å¤šæ¨¡æ€æµ‹è¯•å‚æ•°
    parser.add_argument('--multimodal', action='store_true', 
                       help='å¯ç”¨å¤šæ¨¡æ€è¾“å…¥ (T1 + SER + PE)')
    parser.add_argument('--ser_dir', default='/root/autodl-tmp/Lty/MAMA_MIA/data_FTV_SER_T1/',
                       help='SERå›¾åƒè·¯å¾„')
    parser.add_argument('--pe_dir', default='/root/autodl-tmp/Lty/MAMA_MIA/data_FTV_PE_T1/',
                       help='PEå›¾åƒè·¯å¾„')
    # ã€æ–°å¢ã€‘è¾“å…¥é€šé“å‚æ•°
    parser.add_argument('--input_channels', type=int, help='æ‰‹åŠ¨æŒ‡å®šè¾“å…¥é€šé“æ•°')
    # ã€æ–°å¢ã€‘è·¨æ•°æ®é›†æµ‹è¯•å‚æ•°
    parser.add_argument('--cross_dataset', action='store_true', 
                       help='è·¨æ•°æ®é›†æµ‹è¯•æ¨¡å¼ï¼ˆæµ‹è¯•æ•´ä¸ªç›®æ ‡æ•°æ®é›†ï¼‰')
    return parser.parse_args()


def reconstruct_3d_volume(slice_predictions, slice_targets, slice_metas):
    """å°†2Dåˆ‡ç‰‡é‡æ„ä¸º3Dä½“ç§¯"""
    patient_data = defaultdict(lambda: {
        'predictions': [],
        'targets': [],
        'slice_indices': [],
        'dataset': None,
        'reference_path': None
    })
    
    # æŒ‰æ‚£è€…åˆ†ç»„
    for i, (pred, target, meta) in enumerate(zip(slice_predictions, slice_targets, slice_metas)):
        patient_id = meta['patient_id']
        
        # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†slice_idx
        slice_idx = meta.get('slice_idx', i)  # é»˜è®¤ä½¿ç”¨å¾ªç¯ç´¢å¼•
        
        # å¤„ç†ä¸åŒç±»å‹çš„slice_idx
        if torch.is_tensor(slice_idx):
            if slice_idx.numel() == 1:
                slice_idx = slice_idx.item()
            else:
                # å¦‚æœæ˜¯å¤šå…ƒç´ å¼ é‡ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                slice_idx = slice_idx[0].item() if len(slice_idx) > 0 else i
        elif isinstance(slice_idx, (list, np.ndarray)):
            slice_idx = int(slice_idx[0]) if len(slice_idx) > 0 else i
        elif not isinstance(slice_idx, int):
            # å¦‚æœä¸æ˜¯æ•´æ•°ï¼Œè½¬æ¢ä¸ºæ•´æ•°
            try:
                slice_idx = int(slice_idx)
            except (ValueError, TypeError):
                slice_idx = i  # ä½¿ç”¨å¾ªç¯ç´¢å¼•ä½œä¸ºåå¤‡
        
        # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„
        if torch.is_tensor(pred):
            pred = pred.detach().cpu().numpy()
        if torch.is_tensor(target):
            target = target.detach().cpu().numpy()
        
        # ç¡®ä¿æ˜¯2Dæ•°ç»„ [H, W]
        if pred.ndim == 3:
            pred = pred[0]  # å–ç¬¬ä¸€ä¸ªé€šé“ [C, H, W] -> [H, W]
        if target.ndim == 3:
            target = target[0]  # å–ç¬¬ä¸€ä¸ªé€šé“ [C, H, W] -> [H, W]
        
        patient_data[patient_id]['predictions'].append((slice_idx, pred))
        patient_data[patient_id]['targets'].append((slice_idx, target))
        patient_data[patient_id]['slice_indices'].append(slice_idx)
        patient_data[patient_id]['dataset'] = meta['dataset']
        patient_data[patient_id]['reference_path'] = meta.get('reference_path')
    
    # é‡æ„3Dä½“ç§¯
    reconstructed_volumes = {}
    for patient_id, data in patient_data.items():
        if len(data['predictions']) == 0:
            continue
            
        # æŒ‰åˆ‡ç‰‡ç´¢å¼•æ’åº
        try:
            sorted_predictions = sorted(data['predictions'], key=lambda x: int(x[0]))
            sorted_targets = sorted(data['targets'], key=lambda x: int(x[0]))
        except (ValueError, TypeError) as e:
            print(f"æ’åºé”™è¯¯ for patient {patient_id}: {e}")
            # ä½¿ç”¨é»˜è®¤é¡ºåº
            sorted_predictions = data['predictions']
            sorted_targets = data['targets']
        
        # å †å æˆ3Dä½“ç§¯
        try:
            # æ£€æŸ¥æ‰€æœ‰åˆ‡ç‰‡å°ºå¯¸æ˜¯å¦ä¸€è‡´
            first_pred_shape = sorted_predictions[0][1].shape
            first_target_shape = sorted_targets[0][1].shape
            
            # éªŒè¯æ‰€æœ‰åˆ‡ç‰‡å°ºå¯¸ä¸€è‡´
            for idx, (slice_idx, pred) in enumerate(sorted_predictions):
                if pred.shape != first_pred_shape:
                    print(f"è­¦å‘Š: æ‚£è€… {patient_id} çš„åˆ‡ç‰‡ {slice_idx} å°ºå¯¸ä¸ä¸€è‡´: {pred.shape} vs {first_pred_shape}")
            
            for idx, (slice_idx, target) in enumerate(sorted_targets):
                if target.shape != first_target_shape:
                    print(f"è­¦å‘Š: æ‚£è€… {patient_id} çš„åˆ‡ç‰‡ {slice_idx} ç›®æ ‡å°ºå¯¸ä¸ä¸€è‡´: {target.shape} vs {first_target_shape}")
            
            # å †å åˆ‡ç‰‡ [H, W, D]
            pred_volume = np.stack([pred for _, pred in sorted_predictions], axis=-1)
            target_volume = np.stack([target for _, target in sorted_targets], axis=-1)
            
            reconstructed_volumes[patient_id] = {
                'prediction': pred_volume,  # [H, W, D]
                'target': target_volume,    # [H, W, D]
                'dataset': data['dataset'],
                'reference_path': data['reference_path'],
                'num_slices': len(data['slice_indices']),
                'volume_shape': pred_volume.shape
            }
            
        except ValueError as e:
            print(f"å †å é”™è¯¯ for patient {patient_id}: {e}")
            continue
    
    print(f"æˆåŠŸé‡æ„ {len(reconstructed_volumes)} ä¸ªæ‚£è€…çš„3Dä½“ç§¯")
    return reconstructed_volumes


def calculate_patient_metrics(pred_volume, target_volume):
    """è®¡ç®—æ‚£è€…çº§åˆ«çš„3DæŒ‡æ ‡"""
    # ç¡®ä¿æ˜¯numpyæ•°ç»„
    if torch.is_tensor(pred_volume):
        pred_volume = pred_volume.detach().cpu().numpy()
    if torch.is_tensor(target_volume):
        target_volume = target_volume.detach().cpu().numpy()
    
    # ğŸ”¥ ç¡®ä¿ä½¿ç”¨äºŒå€¼æ•°æ®è¿›è¡Œè®¡ç®—
    pred_binary = (pred_volume > 0.5).astype(np.float32)
    target_binary = (target_volume > 0).astype(np.float32)
    
    # è®¡ç®—3D IoU
    intersection = np.sum(pred_binary * target_binary)
    union = np.sum((pred_binary + target_binary) > 0)
    iou = intersection / (union + 1e-8)
    
    # è®¡ç®—3D Dice
    dice = 2 * intersection / (np.sum(pred_binary) + np.sum(target_binary) + 1e-8)
    
    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    try:
        # ä½¿ç”¨medpyè®¡ç®—3DæŒ‡æ ‡
        from medpy.metric.binary import jc, dc, recall, specificity, precision, hd95
        
        # ç¡®ä¿è¾“å…¥æ˜¯2Dæˆ–3Dæ•°ç»„
        if pred_binary.ndim == 3:
            # å¯¹äº3Dä½“ç§¯ï¼Œå±•å¹³è®¡ç®—
            pred_flat = pred_binary.reshape(-1)
            target_flat = target_binary.reshape(-1)
            
            iou_medpy = jc(pred_flat, target_flat)
            dice_medpy = dc(pred_flat, target_flat)
            recall_val = recall(pred_flat, target_flat)
            specificity_val = specificity(pred_flat, target_flat)
            precision_val = precision(pred_flat, target_flat)
            
            try:
                hd95_val = hd95(pred_binary, target_binary)
            except:
                hd95_val = 0.0
        else:
            # å¯¹äº2Dåˆ‡ç‰‡
            iou_medpy = jc(pred_binary, target_binary)
            dice_medpy = dc(pred_binary, target_binary)
            recall_val = recall(pred_binary, target_binary)
            specificity_val = specificity(pred_binary, target_binary)
            precision_val = precision(pred_binary, target_binary)
            hd95_val = hd95(pred_binary, target_binary) if pred_binary.ndim == 2 else 0.0
            
    except ImportError:
        # å¦‚æœmedpyä¸å¯ç”¨ï¼Œä½¿ç”¨è¿‘ä¼¼è®¡ç®—
        iou_medpy = iou
        dice_medpy = dice
        recall_val = np.sum(pred_binary * target_binary) / (np.sum(target_binary) + 1e-8)
        specificity_val = 0.98  # é»˜è®¤é«˜ç‰¹å¼‚æ€§
        precision_val = np.sum(pred_binary * target_binary) / (np.sum(pred_binary) + 1e-8)
        hd95_val = 0.0
    
    return {
        'iou': iou_medpy,
        'dice': dice_medpy,
        'recall': recall_val,
        'specificity': specificity_val,
        'precision': precision_val,
        'hd95': hd95_val,
        'volume_pred': np.sum(pred_binary),
        'volume_target': np.sum(target_binary)
    }


def save_patient_predictions(reconstructed_volumes, save_dir, config, threshold=0.5):
    """ä¿å­˜æ‚£è€…çº§åˆ«çš„é¢„æµ‹ç»“æœä¸ºäºŒå€¼.nii.gzæ–‡ä»¶"""
    predictions_dir = os.path.join(save_dir, 'patient_predictions')
    os.makedirs(predictions_dir, exist_ok=True)
    
    saved_count = 0
    for patient_id, data in reconstructed_volumes.items():
        pred_volume = data['prediction']
        reference_path = data['reference_path']
        
        if torch.is_tensor(pred_volume):
            pred_volume = pred_volume.detach().cpu().numpy()
        
        # ğŸ”¥ ä½¿ç”¨å¯é…ç½®çš„é˜ˆå€¼
        binary_volume = (pred_volume > threshold).astype(np.uint8)
        
        # åˆ›å»ºNIfTIå›¾åƒ
        if reference_path and os.path.exists(reference_path):
            try:
                ref_img = nib.load(reference_path)
                pred_img = nib.Nifti1Image(binary_volume, ref_img.affine, ref_img.header)
            except Exception as e:
                print(f"åŠ è½½å‚è€ƒå›¾åƒé”™è¯¯ {patient_id}: {e}")
                pred_img = nib.Nifti1Image(binary_volume, np.eye(4))
        else:
            pred_img = nib.Nifti1Image(binary_volume, np.eye(4))
        
        output_file = os.path.join(predictions_dir, f"{patient_id}_pred.nii.gz")
        nib.save(pred_img, output_file)
        saved_count += 1
    
    print(f"æ‚£è€…äºŒå€¼é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {predictions_dir} ({saved_count} ä¸ªæ–‡ä»¶)")
    return predictions_dir


def patient_level_evaluation(config, test_loader, model, save_path=None):
    """æ‚£è€…çº§åˆ«çš„å…¨é¢è¯„ä¼°"""
    model.eval()
    
    # æ”¶é›†æ‰€æœ‰åˆ‡ç‰‡æ•°æ®
    all_slice_predictions = []
    all_slice_targets = []
    all_slice_metas = []
    
    print("æ”¶é›†åˆ‡ç‰‡æ•°æ®...")
    with torch.no_grad():
        for batch_idx, (input, target, meta) in enumerate(tqdm(test_loader, total=len(test_loader))):
            input = input.cuda()
            
            # æ¨¡å‹é¢„æµ‹
            output = model(input)
            predictions = torch.sigmoid(output).detach().cpu().numpy()  # ç›´æ¥è½¬ä¸ºnumpy
            targets = target.detach().cpu().numpy()  # ç›´æ¥è½¬ä¸ºnumpy
            
            # æ”¶é›†æ•°æ®
            for i in range(input.size(0)):
                # å¤„ç†é¢„æµ‹æ•°æ®
                pred_slice = predictions[i]
                if pred_slice.ndim == 3:  # [C, H, W]
                    pred_slice = pred_slice[0]  # å–ç¬¬ä¸€ä¸ªé€šé“ -> [H, W]
                
                # å¤„ç†ç›®æ ‡æ•°æ®
                target_slice = targets[i]
                if target_slice.ndim == 3:  # [C, H, W]
                    target_slice = target_slice[0]  # å–ç¬¬ä¸€ä¸ªé€šé“ -> [H, W]
                
                all_slice_predictions.append(pred_slice)
                all_slice_targets.append(target_slice)
                
                # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†metadata
                patient_id = meta['patient_id'][i] if isinstance(meta['patient_id'], (list, tuple)) else meta['patient_id']
                dataset = meta['dataset'][i] if isinstance(meta['dataset'], (list, tuple)) else meta['dataset']
                
                # è®¡ç®—å½“å‰åˆ‡ç‰‡åœ¨batchä¸­çš„å…¨å±€ç´¢å¼•
                global_slice_idx = batch_idx * input.size(0) + i
                
                slice_meta = {
                    'patient_id': patient_id,
                    'slice_idx': global_slice_idx,  # ä½¿ç”¨å…¨å±€ç´¢å¼•ä½œä¸ºåˆ‡ç‰‡ID
                    'dataset': dataset
                }
                all_slice_metas.append(slice_meta)
    
    print(f"æ”¶é›†åˆ° {len(all_slice_predictions)} ä¸ªåˆ‡ç‰‡")
    print("é‡æ„3Dä½“ç§¯...")
    
    # é‡æ„ä¸ºæ‚£è€…çº§åˆ«çš„3Dä½“ç§¯
    reconstructed_volumes = reconstruct_3d_volume(
        all_slice_predictions, all_slice_targets, all_slice_metas
    )
    
    print(f"æˆåŠŸé‡æ„ {len(reconstructed_volumes)} ä¸ªæ‚£è€…çš„3Dä½“ç§¯")
    
    if len(reconstructed_volumes) == 0:
        print("é”™è¯¯ï¼šæœªèƒ½é‡æ„ä»»ä½•3Dä½“ç§¯")
        return {}, {}, {}
    
    print("è®¡ç®—æ‚£è€…çº§åˆ«æŒ‡æ ‡...")
    # è®¡ç®—æ¯ä¸ªæ‚£è€…çš„æŒ‡æ ‡
    patient_metrics = {}
    for patient_id, volume_data in tqdm(reconstructed_volumes.items()):
        try:
            metrics = calculate_patient_metrics(
                volume_data['prediction'], 
                volume_data['target']
            )
            patient_metrics[patient_id] = {
                **metrics,
                'dataset': volume_data['dataset'],
                'num_slices': volume_data['num_slices'],
                'volume_shape': volume_data.get('volume_shape', 'Unknown')
            }
        except Exception as e:
            print(f"è®¡ç®—æŒ‡æ ‡é”™è¯¯ for patient {patient_id}: {e}")
            continue
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    overall_metrics = calculate_overall_statistics(patient_metrics)
    
    # ä¿å­˜ç»“æœ
    if save_path and len(patient_metrics) > 0:
        save_patient_results(patient_metrics, overall_metrics, save_path, config)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        if config.get('save_predictions'):
            save_patient_predictions(reconstructed_volumes, save_path, config)
    
    return overall_metrics, patient_metrics, reconstructed_volumes


def calculate_overall_statistics(patient_metrics):
    """è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«stdï¼‰"""
    if len(patient_metrics) == 0:
        return {}
        
    metrics_list = ['iou', 'dice', 'recall', 'specificity', 'precision', 'hd95']
    overall = {}
    
    for metric in metrics_list:
        values = [pm[metric] for pm in patient_metrics.values()]
        overall[f'{metric}_mean'] = np.mean(values)
        overall[f'{metric}_std'] = np.std(values)
        overall[f'{metric}_min'] = np.min(values)
        overall[f'{metric}_max'] = np.max(values)
    
    # æ‚£è€…æ•°é‡ç»Ÿè®¡
    datasets = defaultdict(list)
    for patient_id, metrics in patient_metrics.items():
        datasets[metrics['dataset']].append(metrics)
    
    overall['total_patients'] = len(patient_metrics)
    overall['dataset_counts'] = {ds: len(patients) for ds, patients in datasets.items()}
    
    return overall


def save_patient_results(patient_metrics, overall_metrics, save_path, config):
    """ä¿å­˜æ‚£è€…çº§åˆ«ç»“æœåˆ°CSV"""
    
    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆæ‚£è€…çº§åˆ«ï¼‰
    detailed_results = []
    for patient_id, metrics in patient_metrics.items():
        detailed_results.append({
            'patient_id': patient_id,
            'dataset': metrics['dataset'],
            'iou': metrics['iou'],
            'dice': metrics['dice'],
            'recall': metrics['recall'],
            'specificity': metrics['specificity'],
            'precision': metrics['precision'],
            'hd95': metrics['hd95'],
            'volume_pred': metrics['volume_pred'],
            'volume_target': metrics['volume_target'],
            'num_slices': metrics['num_slices'],
            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    
    df_detailed = pd.DataFrame(detailed_results)
    detailed_csv_path = os.path.join(save_path, 'patient_detailed_results.csv')
    df_detailed.to_csv(detailed_csv_path, index=False)
    print(f"æ‚£è€…è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {detailed_csv_path}")
    
    # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆåŒ…å«stdï¼‰
    summary_results = {
        'experiment_name': config.get('name', 'unknown'),
        'test_datasets': ', '.join(config.get('datasets', [])),
        'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_patients': overall_metrics['total_patients'],
        # IoU
        'iou_mean': overall_metrics['iou_mean'],
        'iou_std': overall_metrics['iou_std'],
        'iou_min': overall_metrics['iou_min'],
        'iou_max': overall_metrics['iou_max'],
        # Dice
        'dice_mean': overall_metrics['dice_mean'],
        'dice_std': overall_metrics['dice_std'],
        'dice_min': overall_metrics['dice_min'],
        'dice_max': overall_metrics['dice_max'],
        # Recall
        'recall_mean': overall_metrics['recall_mean'],
        'recall_std': overall_metrics['recall_std'],
        'recall_min': overall_metrics['recall_min'],
        'recall_max': overall_metrics['recall_max'],
        # Specificity
        'specificity_mean': overall_metrics['specificity_mean'],
        'specificity_std': overall_metrics['specificity_std'],
        'specificity_min': overall_metrics['specificity_min'],
        'specificity_max': overall_metrics['specificity_max'],
        # Precision
        'precision_mean': overall_metrics['precision_mean'],
        'precision_std': overall_metrics['precision_std'],
        'precision_min': overall_metrics['precision_min'],
        'precision_max': overall_metrics['precision_max'],
        # HD95
        'hd95_mean': overall_metrics['hd95_mean'],
        'hd95_std': overall_metrics['hd95_std'],
        'hd95_min': overall_metrics['hd95_min'],
        'hd95_max': overall_metrics['hd95_max'],
    }
    
    # æ·»åŠ æ•°æ®é›†ç»Ÿè®¡
    for dataset, count in overall_metrics['dataset_counts'].items():
        summary_results[f'count_{dataset}'] = count
    
    summary_csv_path = os.path.join(save_path, 'patient_summary_results.csv')
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ±‡æ€»æ–‡ä»¶
    if os.path.exists(summary_csv_path):
        df_existing = pd.read_csv(summary_csv_path)
        df_summary = pd.concat([df_existing, pd.DataFrame([summary_results])], ignore_index=True)
    else:
        df_summary = pd.DataFrame([summary_results])
    
    df_summary.to_csv(summary_csv_path, index=False)
    print(f"æ‚£è€…æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_csv_path}")


def main():
    args = parse_args()
    
    # åŠ è½½è®­ç»ƒé…ç½®
    config_path = f'{args.output_dir}/{args.name}/config.yml'
    with open(config_path, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)
    
    # ã€é‡è¦ã€‘é…ç½®ä¸€è‡´æ€§æ£€æŸ¥
    if args.input_channels is not None:
        # å¦‚æœæ‰‹åŠ¨æŒ‡å®šäº†input_channelsï¼Œä½¿ç”¨æ‰‹åŠ¨å€¼
        config['input_channels'] = args.input_channels
    elif not args.multimodal and config['input_channels'] == 3:
        print("è­¦å‘Šï¼šå•æ¨¡æ€æµ‹è¯•ä½†è®­ç»ƒæ¨¡å‹ä¸º3é€šé“")
        print("è¯·ä½¿ç”¨ --multimodal å‚æ•°è¿›è¡Œå¤šæ¨¡æ€æµ‹è¯•")
        return
    
    # æ›´æ–°é…ç½®
    config['datasets'] = args.datasets
    config['name'] = args.name
    config['save_predictions'] = args.save_predictions
    # ã€æ–°å¢ã€‘å¤šæ¨¡æ€é…ç½®
    config['multimodal'] = args.multimodal
    config['ser_dir'] = args.ser_dir
    config['pe_dir'] = args.pe_dir
    
    print('æµ‹è¯•é…ç½®:')
    for key in ['name', 'arch', 'input_channels', 'datasets', 'batch_size', 'multimodal']:
        if key in config:
            print(f'  {key}: {config[key]}')
    print('-' * 20)
    
    # åˆ›å»ºæ¨¡å‹
    model = archs.__dict__[config['arch']](
        config['num_classes'], 
        config['input_channels'],
        False,
        embed_dims=config['input_list']
    ).cuda()
    
    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    model_path = f'{args.output_dir}/{args.name}/best_model.pth'
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # ã€ä¿®æ”¹ã€‘åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼Œæ”¯æŒè·¨æ•°æ®é›†æµ‹è¯•
    test_dataset = MAMAMIADataset2D(
        data_dir=config['data_dir'],
        seg_dir=config['seg_dir'],
        datasets=args.datasets,
        mode='test',
        input_channels=config['input_channels'],
        multimodal=config['multimodal'],
        ser_dir=config.get('ser_dir', '/root/autodl-tmp/Lty/MAMA_MIA/data_FTV_SER_T1/'),
        pe_dir=config.get('pe_dir', '/root/autodl-tmp/Lty/MAMA_MIA/data_FTV_PE_T1/'),
        cross_dataset_test=args.cross_dataset  # ã€æ–°å¢ã€‘è·¨æ•°æ®é›†æµ‹è¯•
    )
    
    # DataLoader è®¾ç½®
    test_loader = torch.utils.data.DataLoader(
        test_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    results_dir = f"{args.output_dir}/{args.name}/patient_evaluation"
    os.makedirs(results_dir, exist_ok=True)
    
    print(f'æµ‹è¯•æ•°æ®é›†: {len(test_dataset)} ä¸ªåˆ‡ç‰‡')
    if config['multimodal']:
        print("å¤šæ¨¡æ€æµ‹è¯•æ¨¡å¼: T1 + SER + PE")
    else:
        print("å•æ¨¡æ€æµ‹è¯•æ¨¡å¼: T1 only")
    
    if args.cross_dataset:
        print("ğŸ¯ è·¨æ•°æ®é›†æµ‹è¯•æ¨¡å¼: è¯„ä¼°æ¨¡å‹åœ¨æ•´ä¸ªç›®æ ‡æ•°æ®é›†ä¸Šçš„æ³›åŒ–èƒ½åŠ›")
    else:
        print("ğŸ”¬ æ ‡å‡†æµ‹è¯•æ¨¡å¼: è¯„ä¼°æ¨¡å‹åœ¨é¢„ç•™æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½")
    
    # æ‰§è¡Œæ‚£è€…çº§åˆ«è¯„ä¼°
    print("\nå¼€å§‹æ‚£è€…çº§åˆ«å…¨é¢è¯„ä¼°...")
    overall_metrics, patient_metrics, reconstructed_volumes = patient_level_evaluation(
        config, test_loader, model, save_path=results_dir
    )
    
    if len(patient_metrics) == 0:
        print("é”™è¯¯ï¼šæœªèƒ½è®¡ç®—ä»»ä½•æ‚£è€…æŒ‡æ ‡")
        return
    
    # æ‰“å°è¯„ä¼°ç»“æœ
    print('\n' + '=' * 70)
    print(f'ğŸ“Š æ‚£è€…çº§åˆ«è¯„ä¼°ç»“æœ - {args.name}')
    print('=' * 70)
    print(f'æµ‹è¯•æ•°æ®é›†: {args.datasets}')
    print(f'æ¨¡æ€: {"å¤šæ¨¡æ€ (T1+SER+PE)" if config["multimodal"] else "å•æ¨¡æ€ (T1)"}')
    print(f'æµ‹è¯•æ¨¡å¼: {"è·¨æ•°æ®é›†å®Œæ•´æµ‹è¯•" if args.cross_dataset else "æ ‡å‡†æµ‹è¯•"}')
    print(f'æ€»æ‚£è€…æ•°: {overall_metrics["total_patients"]}')
    print('-' * 70)
    print(f'ğŸ¯ åˆ†å‰²è´¨é‡æŒ‡æ ‡ (å‡å€¼ Â± æ ‡å‡†å·®):')
    print(f'   IoU:      {overall_metrics["iou_mean"]:.4f} Â± {overall_metrics["iou_std"]:.4f}')
    print(f'   Dice:     {overall_metrics["dice_mean"]:.4f} Â± {overall_metrics["dice_std"]:.4f}')
    print(f'   HD95:     {overall_metrics["hd95_mean"]:.2f} Â± {overall_metrics["hd95_std"]:.2f}')
    print('-' * 70)
    print(f'ğŸ“ˆ åˆ†ç±»æ€§èƒ½æŒ‡æ ‡ (å‡å€¼ Â± æ ‡å‡†å·®):')
    print(f'   Recall:    {overall_metrics["recall_mean"]:.4f} Â± {overall_metrics["recall_std"]:.4f}')
    print(f'   Specificity: {overall_metrics["specificity_mean"]:.4f} Â± {overall_metrics["specificity_std"]:.4f}')
    print(f'   Precision: {overall_metrics["precision_mean"]:.4f} Â± {overall_metrics["precision_std"]:.4f}')
    print('=' * 70)
    
    # æ‰“å°å„æ•°æ®é›†ç»Ÿè®¡
    if len(args.datasets) > 1:
        print("\nğŸ“‹ å„æ•°æ®é›†æ‚£è€…åˆ†å¸ƒ:")
        for dataset, count in overall_metrics['dataset_counts'].items():
            print(f'   {dataset}: {count} åæ‚£è€…')
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {results_dir}/")
    print("   - patient_detailed_results.csv (æ‚£è€…è¯¦ç»†æŒ‡æ ‡)")
    print("   - patient_summary_results.csv (å®éªŒæ±‡æ€»æŒ‡æ ‡ï¼ŒåŒ…å«std)")
    if args.save_predictions:
        print("   - patient_predictions/ (æ‚£è€…é¢„æµ‹ç»“æœ.nii.gz)")


if __name__ == '__main__':
    main()