"""
äº”æŠ˜äº¤å‰éªŒè¯è„šæœ¬
"""

import os
import argparse
import numpy as np
import random
import torch
from torch.utils.data import DataLoader, Subset
import pandas as pd
from collections import defaultdict
import shutil
from datetime import datetime
import json
import sys
import time
import warnings
warnings.filterwarnings('ignore', message='upsample_bilinear2d_backward_out_cuda')

from utils import set_seed, get_logger, seed_data_loader, BceDiceLoss, get_optimizer, get_scheduler
from config_setting_mama_mia import MamaMiaConfig
from mama_mia_loader import MAMAMIADataLoader
from engine import train_one_epoch, val_one_epoch, test_one_epoch

try:
    from models.ultralight_vm_unet_enhanced import create_ultralight_model
    USE_ENHANCED_MODEL = True
    print("âœ… Enhanced model module found")
except ImportError:
    # å¦‚æœå¢å¼ºç‰ˆæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
    USE_ENHANCED_MODEL = False
    print("âš ï¸ Enhanced model module not found")

# å¯¼å…¥thopç”¨äºè®¡ç®—FLOPs
try:
    from thop import profile, clever_format
    HAS_THOP = True
except ImportError:
    HAS_THOP = False
    print("âš ï¸ thop module not found, cannot calculate FLOPs")

from mama_mia_dataset import MAMAMIADataset2D, MAMAMIAMultiModalAugmentation


def parse_args():
    parser = argparse.ArgumentParser(description='MAMA-MIA äº”æŠ˜äº¤å‰éªŒè¯')
    
    parser.add_argument('--model', type=str, default='ultralight', 
                       choices=['ultralight', 'ultralight_enhanced', 'unet', 'attention_unet', 
                               'unet_plusplus', 'deeplabv3', 'swin_unet', 'nnunet', 
                               'transunet', 'fcn'],
                       help='é€‰æ‹©æ¨¡å‹ç±»å‹: ultralight(ä½ çš„åŸå§‹æ¨¡å‹), ultralight_enhanced(å¢å¼ºç‰ˆ), unetç­‰')
    
    # å®éªŒé…ç½®
    parser.add_argument('--name', type=str, default='cv_experiment', help='å®éªŒåç§°')
    parser.add_argument('--k_folds', type=int, default=5, help='äº¤å‰éªŒè¯æŠ˜æ•°')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    
    # æ•°æ®é›†é…ç½®
    parser.add_argument('--datasets', nargs='+', default=['DUKE', 'NACT', 'ISPY1', 'ISPY2'],
                       help='ä½¿ç”¨çš„æ•°æ®é›†åˆ—è¡¨')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--multimodal', action='store_true', 
                       help='ä½¿ç”¨å¤šæ¨¡æ€è¾“å…¥ (T1 + SER + PE)')
    parser.add_argument('--input_channels', type=int, default=1, 
                       help='è¾“å…¥é€šé“æ•°')
    
    # åŠ¨æ€èåˆå‚æ•°
    parser.add_argument('--enable_fusion', action='store_true', 
                       help='Enable dynamic modal fusion (requires multimodal)')
    parser.add_argument('--fusion_verbose', action='store_true',
                       help='Enable verbose output for fusion module')
    parser.add_argument('--test_weight_method', type=str, default='historical_mean',
                       choices=['current', 'historical_mean', 'historical_median', 'last'],
                       help='Test weight selection method for dynamic fusion')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=100, help='æ¯æŠ˜è®­ç»ƒçš„epochæ•°')
    parser.add_argument('--batch_size', type=int, default=256, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    
    # æ•°æ®å¢å¼º
    parser.add_argument('--use_augmentation', action='store_true',
                       help='ä½¿ç”¨æ•°æ®å¢å¼º')
    parser.add_argument('--balanced_sampling', action='store_true',
                       help='ä½¿ç”¨å¹³è¡¡é‡‡æ ·')
    
    # ä¼˜åŒ–å™¨é…ç½®
    parser.add_argument('--opt', type=str, default='AdamW', help='ä¼˜åŒ–å™¨ç±»å‹')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    parser.add_argument('--sch', type=str, default='CosineAnnealingLR', help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    parser.add_argument('--T_max', type=int, default=50, help='CosineAnnealingLRçš„T_max')
    
    # æµ‹è¯•é…ç½®
    parser.add_argument('--cross_dataset_test', action='store_true',
                       help='æ˜¯å¦è¿›è¡Œè·¨æ•°æ®é›†æµ‹è¯•')
    parser.add_argument('--test_datasets', nargs='+', 
                       help='è·¨æ•°æ®é›†æµ‹è¯•æ—¶ä½¿ç”¨çš„æµ‹è¯•æ•°æ®é›†')
    
    parser.add_argument('--fold_indices', type=int, nargs='+', default=None,
                       help='æŒ‡å®šè¦è®­ç»ƒçš„æŠ˜æ•°ç´¢å¼•ï¼ˆ0-basedï¼‰ï¼Œä¾‹å¦‚ï¼š0 æˆ– 0 2 4ã€‚ä¸æŒ‡å®šåˆ™è®­ç»ƒæ‰€æœ‰æŠ˜')
    
    
    return parser.parse_args()


class MAMAMIADatasetCV(MAMAMIADataset2D):
    """ä¸“é—¨ç”¨äºäº¤å‰éªŒè¯çš„æ•°æ®é›†ï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®"""
    
    def __init__(self, **kwargs):
        # ä¿å­˜åŸå§‹å‚æ•°ç”¨äºè°ƒè¯•
        original_kwargs = kwargs.copy()
        
        # ç§»é™¤MAMAMIADataset2Dä¸æ”¯æŒçš„å‚æ•°
        if 'train_ratio' in kwargs:
            kwargs.pop('train_ratio')
        if 'val_ratio' in kwargs:
            kwargs.pop('val_ratio')
        
        # è®¾ç½®modeä¸º'train'ä»¥è·å–æ‰€æœ‰æ•°æ®
        kwargs['mode'] = 'train'
        kwargs['cross_dataset_test'] = False
        
        print(f"åˆ›å»ºäº¤å‰éªŒè¯æ•°æ®é›†...")
        
        super().__init__(**kwargs)
        
        # éªŒè¯æˆ‘ä»¬ç¡®å®ä½¿ç”¨äº†æ‰€æœ‰æ‚£è€…
        print(f"äº¤å‰éªŒè¯æ•°æ®é›†: ä½¿ç”¨æ‰€æœ‰ {len(self.original_dataset.patient_ids)} ä¸ªæ‚£è€…")
        print(f"åˆ‡ç‰‡æ€»æ•°: {len(self)}")


class KFoldSplitter:
    """äº”æŠ˜äº¤å‰éªŒè¯æ•°æ®åˆ†å‰²å™¨"""
    
    def __init__(self, dataset, n_splits=5, seed=42, show_progress=True):
        self.dataset = dataset
        self.n_splits = n_splits
        self.seed = seed
        self._show_progress = show_progress
        
        print(f"\n{'='*50}")
        print(f"åˆ›å»º {n_splits} æŠ˜äº¤å‰éªŒè¯åˆ†å‰²å™¨")
        print(f"{'='*50}")
        
        # è·å–æ‰€æœ‰æ‚£è€…ID
        self.patient_ids = list(dataset.original_dataset.patient_data.keys())
        self.patient_ids.sort()  # æ’åºä»¥ç¡®ä¿å¯é‡å¤æ€§
        self.total_patients = len(self.patient_ids)
        
        print(f"æ‚£è€…æ€»æ•°: {self.total_patients}")
        print(f"åˆ‡ç‰‡æ€»æ•°: {len(self.dataset)}")
        
        # å¿«é€Ÿåˆ›å»ºæ˜ å°„
        self._create_patient_to_slices_mapping()
        
        # ç”ŸæˆæŠ˜æ•°åˆ’åˆ†
        self.folds = self._generate_folds()
        
        # éªŒè¯åˆ’åˆ†
        self._validate_folds()
        
        print(f"\nâœ… KFold Splitter åˆ›å»ºå®Œæˆ")
        print(f"{'='*50}")
    
    def _create_patient_to_slices_mapping(self):
        """åˆ›å»ºæ‚£è€…IDåˆ°æ‰€æœ‰åˆ‡ç‰‡ç´¢å¼•çš„æ˜ å°„"""
        print(f"\n[æ­¥éª¤1] åˆ›å»ºæ‚£è€…åˆ°åˆ‡ç‰‡æ˜ å°„...")
        
        self.patient_slices = defaultdict(list)
        self.patient_slice_counts = {}
        
        # ä½¿ç”¨åŸå§‹æ•°æ®é›†çš„slice_indices
        original_patient_ids = self.dataset.original_dataset.patient_ids
        total_slices = len(self.dataset)
        
        if self._show_progress:
            print(f"  æ­£åœ¨å¤„ç† {total_slices} ä¸ªåˆ‡ç‰‡...")
        
        start_time = time.time()
        
        # å¿«é€Ÿéå†
        for slice_idx in range(total_slices):
            patient_idx, _ = self.dataset.slice_indices[slice_idx]
            
            # æ˜¾ç¤ºè¿›åº¦
            if self._show_progress and slice_idx % 1000 == 0 and slice_idx > 0:
                elapsed = time.time() - start_time
                progress = (slice_idx + 1) / total_slices * 100
                print(f"  è¿›åº¦: {slice_idx+1}/{total_slices} ({progress:.1f}%) - å·²ç”¨ {elapsed:.1f}ç§’")
            
            if patient_idx < len(original_patient_ids):
                patient_id = original_patient_ids[patient_idx]
                self.patient_slices[patient_id].append(slice_idx)
            else:
                print(f"  è­¦å‘Š: patient_idx {patient_idx} è¶…å‡ºèŒƒå›´")
        
        # ç»Ÿè®¡
        for patient_id, slices in self.patient_slices.items():
            self.patient_slice_counts[patient_id] = len(slices)
        
        elapsed = time.time() - start_time
        print(f"  å®Œæˆ: {len(self.patient_slices)} ä¸ªæ‚£è€…ï¼Œ{total_slices} ä¸ªåˆ‡ç‰‡")
        print(f"  è€—æ—¶: {elapsed:.2f} ç§’")
    
    def _generate_folds(self):
        """ç”Ÿæˆäº”æŠ˜åˆ’åˆ†"""
        print(f"\n[æ­¥éª¤2] ç”Ÿæˆ {self.n_splits} æŠ˜åˆ’åˆ†...")
        
        # åªä½¿ç”¨æœ‰åˆ‡ç‰‡çš„æ‚£è€…
        patients_with_slices = [pid for pid in self.patient_ids 
                               if pid in self.patient_slices and len(self.patient_slices[pid]) > 0]
        
        print(f"  æœ‰åˆ‡ç‰‡çš„æ‚£è€…: {len(patients_with_slices)}/{self.total_patients}")
        
        if len(patients_with_slices) == 0:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰åˆ‡ç‰‡çš„æ‚£è€…ï¼")
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.seed)
        
        # éšæœºæ‰“ä¹±æ‚£è€…ID
        shuffled_patient_ids = patients_with_slices.copy()
        np.random.shuffle(shuffled_patient_ids)
        
        # è®¡ç®—æ¯æŠ˜çš„æ‚£è€…æ•°é‡
        patients_per_fold = len(shuffled_patient_ids) // self.n_splits
        remainder = len(shuffled_patient_ids) % self.n_splits
        
        folds = []
        start_idx = 0
        
        for fold_idx in range(self.n_splits):
            # è®¡ç®—è¯¥æŠ˜çš„æ‚£è€…æ•°é‡
            fold_patient_count = patients_per_fold + (1 if fold_idx < remainder else 0)
            
            # è·å–è¯¥æŠ˜çš„æ‚£è€…ID
            fold_patient_ids = shuffled_patient_ids[start_idx:start_idx + fold_patient_count]
            
            # è·å–è¿™äº›æ‚£è€…çš„æ‰€æœ‰åˆ‡ç‰‡ç´¢å¼•
            fold_slice_indices = []
            for patient_id in fold_patient_ids:
                fold_slice_indices.extend(self.patient_slices[patient_id])
            
            folds.append({
                'fold_idx': fold_idx,
                'patient_ids': fold_patient_ids,
                'slice_indices': fold_slice_indices,
                'num_patients': len(fold_patient_ids),
                'num_slices': len(fold_slice_indices)
            })
            
            start_idx += fold_patient_count
        
        return folds
    
    def _validate_folds(self):
        """éªŒè¯åˆ’åˆ†ç»“æœ"""
        print(f"\n[æ­¥éª¤3] éªŒè¯åˆ’åˆ†ç»“æœ...")
        
        total_slices_in_folds = 0
        
        for fold in self.folds:
            print(f"  æŠ˜ {fold['fold_idx']}: {fold['num_patients']} æ‚£è€…, {fold['num_slices']} åˆ‡ç‰‡")
            total_slices_in_folds += fold['num_slices']
        
        print(f"\n  æ€»åˆ‡ç‰‡æ•°: {len(self.dataset)}")
        print(f"  å·²åˆ†é…åˆ‡ç‰‡: {total_slices_in_folds}")
        
        if total_slices_in_folds != len(self.dataset):
            print(f"  è­¦å‘Š: æœ‰ {len(self.dataset) - total_slices_in_folds} ä¸ªåˆ‡ç‰‡æœªåˆ†é…!")
        else:
            print(f"  âœ… æ‰€æœ‰åˆ‡ç‰‡å·²æ­£ç¡®åˆ†é…")
    
    def get_fold_data(self, fold_idx):
        """è·å–æŒ‡å®šæŠ˜çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ç´¢å¼•"""
        if fold_idx >= self.n_splits:
            raise ValueError(f"Fold index {fold_idx} out of range (0-{self.n_splits-1})")
        
        # éªŒè¯é›†æ˜¯å½“å‰æŠ˜
        val_slice_indices = self.folds[fold_idx]['slice_indices']
        
        if len(val_slice_indices) == 0:
            print(f"è­¦å‘Š: æŠ˜ {fold_idx} çš„éªŒè¯é›†æœ‰0ä¸ªåˆ‡ç‰‡!")
        
        # è®­ç»ƒé›†æ˜¯æ‰€æœ‰å…¶ä»–æŠ˜
        train_slice_indices = []
        for i, fold in enumerate(self.folds):
            if i != fold_idx:
                train_slice_indices.extend(fold['slice_indices'])
        
        if len(train_slice_indices) == 0:
            print(f"è­¦å‘Š: æŠ˜ {fold_idx} çš„è®­ç»ƒé›†æœ‰0ä¸ªåˆ‡ç‰‡!")
        
        return train_slice_indices, val_slice_indices


def create_cross_validation_datasets(config, fold_splitter, fold_idx):
    """åˆ›å»ºäº¤å‰éªŒè¯çš„æ•°æ®é›†å’ŒåŠ è½½å™¨"""
    # è·å–å½“å‰æŠ˜çš„è®­ç»ƒ/éªŒè¯åˆ‡ç‰‡ç´¢å¼•
    train_indices, val_indices = fold_splitter.get_fold_data(fold_idx)
    
    print(f"\nFold {fold_idx} Dataset Info:")
    print(f"  Train slices: {len(train_indices)}")
    print(f"  Val slices: {len(val_indices)}")
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
    if len(train_indices) == 0:
        print(f"é”™è¯¯: æŠ˜ {fold_idx} çš„è®­ç»ƒé›†ä¸ºç©º!")
        return None, None
    if len(val_indices) == 0:
        print(f"é”™è¯¯: æŠ˜ {fold_idx} çš„éªŒè¯é›†ä¸ºç©º!")
        return None, None
    
    # åˆ›å»ºSubsetæ•°æ®é›†
    train_subset = Subset(fold_splitter.dataset, train_indices)
    val_subset = Subset(fold_splitter.dataset, val_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_subset,
        batch_size=config.batch_size,
        shuffle=True,  # è®­ç»ƒé›†éœ€è¦shuffle
        num_workers=min(config.num_workers, 2),  # é™åˆ¶çº¿ç¨‹æ•°
        pin_memory=True,
        drop_last=True if len(train_indices) > config.batch_size else False
    )
    
    val_loader = DataLoader(
        val_subset,
        batch_size=1,  # éªŒè¯æ—¶batch_size=1
        shuffle=False,
        num_workers=min(config.num_workers, 2),
        pin_memory=True,
        drop_last=False
    )
    
    # åº”ç”¨éšæœºç§å­
    train_loader = seed_data_loader(train_loader, config.seed + fold_idx)
    val_loader = seed_data_loader(val_loader, config.seed + fold_idx)
    
    return train_loader, val_loader


def create_model_by_type(model_type, config):
    """æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹"""
    print(f"æ­£åœ¨åˆ›å»º {model_type.upper()} æ¨¡å‹...")
    
    # å¯¼å…¥éœ€è¦çš„æ¨¡å—
    import sys
    import os
    
    # è·å–modelsç›®å½•è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    models_dir = os.path.join(current_dir, 'models')
    
    # ç¡®ä¿modelsç›®å½•åœ¨Pythonè·¯å¾„ä¸­
    if models_dir not in sys.path:
        sys.path.insert(0, models_dir)
    
    try:
        # ==================== ã€ä½ çš„æ¨¡å‹ - éœ€è¦c_listå‚æ•°ã€‘ ====================
        if model_type in ['ultralight', 'ultralight_enhanced']:
            if model_type == 'ultralight':
                from UltraLight_VM_UNet import UltraLight_VM_UNet
                model = UltraLight_VM_UNet(
                    num_classes=config.model_config['num_classes'],
                    input_channels=config.model_config['input_channels'],
                    c_list=config.model_config['c_list'],
                    split_att=config.model_config['split_att'],
                    bridge=config.model_config['bridge'],
                )
                model_type_display = "UltraLight VM-UNet"
                print(f"  âœ… ä½ çš„æ¨¡å‹: c_list={config.model_config['c_list']}")
                
            elif model_type == 'ultralight_enhanced':
                if USE_ENHANCED_MODEL:
                    from ultralight_vm_unet_enhanced import create_ultralight_model
                    model = create_ultralight_model(config)
                    model_type_display = "Enhanced UltraLight VM-UNet"
                else:
                    from UltraLight_VM_UNet import UltraLight_VM_UNet
                    model = UltraLight_VM_UNet(
                        num_classes=config.model_config['num_classes'],
                        input_channels=config.model_config['input_channels'],
                        c_list=config.model_config['c_list'],
                        split_att=config.model_config['split_att'],
                        bridge=config.model_config['bridge'],
                    )
                    model_type_display = "UltraLight VM-UNet (å¢å¼ºç‰ˆä¸å¯ç”¨)"
                print(f"  âœ… ä½ çš„å¢å¼ºæ¨¡å‹")
        
        elif model_type == 'unet':
            # æ ‡å‡†UNet - ä½¿ç”¨é»˜è®¤é…ç½®
            from baseline_unet import Baseline_UNet
            
            print(f"  âœ… æ ‡å‡†UNet: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            
            # å…³é”®ï¼šæ ‡å‡†UNetåº”è¯¥ä½¿ç”¨è‡ªå·±çš„é»˜è®¤å‚æ•°ï¼Œä¸ä¼ é€’c_list
            model = Baseline_UNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°ï¼è®©æ¨¡å‹ä½¿ç”¨è‡ªå·±çš„é»˜è®¤å€¼
            )
            model_type_display = "Standard UNet"
            
        elif model_type == 'attention_unet':
            from baseline_attention_unet import Baseline_Attention_UNet
            print(f"  âœ… æ ‡å‡†Attention UNet: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_Attention_UNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard Attention UNet"
            
        elif model_type == 'unet_plusplus':
            from baseline_unet_plusplus import Baseline_UNetPlusPlus
            print(f"  âœ… æ ‡å‡†UNet++: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_UNetPlusPlus(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                deep_supervision=False,  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard UNet++"
            
        elif model_type == 'deeplabv3':
            from baseline_deeplabv3 import Baseline_DeeplabV3
            print(f"  âœ… æ ‡å‡†DeeplabV3: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_DeeplabV3(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard DeeplabV3"
            
        elif model_type == 'swin_unet':
            from baseline_swin_unet import Baseline_Swin_UNet
            print(f"  âœ… æ ‡å‡†Swin UNet: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_Swin_UNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard Swin UNet"
            
        elif model_type == 'nnunet':
            from baseline_nnunet import Baseline_nnUNet
            print(f"  âœ… æ ‡å‡†nnUNet: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_nnUNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard nnUNet"
            
        elif model_type == 'transunet':
            from baseline_transunet import Baseline_TransUNet
            print(f"  âœ… æ ‡å‡†TransUNet: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_TransUNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard TransUNet"
            
        elif model_type == 'fcn':
            from baseline_fcn import Baseline_FCN
            print(f"  âœ… æ ‡å‡†FCN: ä½¿ç”¨é»˜è®¤æ ‡å‡†é…ç½®")
            model = Baseline_FCN(
                num_classes=config.model_config['num_classes'],
                input_channels=config.input_channels,
                # ä¸ä¼ é€’c_listå‚æ•°
            )
            model_type_display = "Standard FCN"
            
        else:
            print(f"âš ï¸ æœªçŸ¥æ¨¡å‹ç±»å‹ '{model_type}'ï¼Œä½¿ç”¨é»˜è®¤UltraLight VM-UNet")
            from UltraLight_VM_UNet import UltraLight_VM_UNet
            model = UltraLight_VM_UNet(
                num_classes=config.model_config['num_classes'],
                input_channels=config.model_config['input_channels'],
                c_list=config.model_config['c_list'],
                split_att=config.model_config['split_att'],
                bridge=config.model_config['bridge'],
            )
            model_type_display = "UltraLight VM-UNet (é»˜è®¤)"
            print(f"  âœ… é»˜è®¤ä½¿ç”¨ä½ çš„æ¨¡å‹: c_list={config.model_config['c_list']}")
    
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å‹å¤±è´¥: {e}")
        print("ä½¿ç”¨UltraLight VM-UNetä½œä¸ºæ›¿ä»£")
        from UltraLight_VM_UNet import UltraLight_VM_UNet
        model = UltraLight_VM_UNet(
            num_classes=config.model_config['num_classes'],
            input_channels=config.model_config['input_channels'],
            c_list=config.model_config['c_list'],
            split_att=config.model_config['split_att'],
            bridge=config.model_config['bridge'],
        )
        model_type_display = f"UltraLight VM-UNet ({model_type}æ›¿ä»£)"
        print(f"  âœ… æ›¿ä»£ä½¿ç”¨ä½ çš„æ¨¡å‹: c_list={config.model_config['c_list']}")
    
    print(f"âœ… {model_type_display} æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # èåˆé…ç½®æ˜¾ç¤º
    if hasattr(config, 'multimodal') and config.multimodal and hasattr(config, 'enable_fusion') and config.enable_fusion:
        print(f"ğŸ¯ Dynamic Fusion: âœ… ENABLED")
    
    return model, model_type_display


def calculate_model_complexity(model, input_channels=3, image_size=256):
    """è®¡ç®—æ¨¡å‹çš„å‚æ•°é‡ - æ ¼å¼åŒ–æ˜¾ç¤ºç‰ˆæœ¬"""
    print("\n#----------Model Complexity Analysis----------#")
    
    try:
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        def format_number(num):
            if num >= 1_000_000:
                return f"{num / 1_000_000:.2f}M ({num:,})"
            elif num >= 1_000:
                return f"{num / 1_000:.2f}K ({num:,})"
            else:
                return f"{num:,}"
        
        print(f"Total Parameters: {format_number(total_params)}")
        print(f"Trainable Parameters: {format_number(trainable_params)}")
        
        # è®¡ç®—éè®­ç»ƒå‚æ•°
        non_trainable_params = total_params - trainable_params
        if non_trainable_params > 0:
            print(f"Non-trainable Parameters: {format_number(non_trainable_params)}")
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        if total_params > 0:
            trainable_percent = (trainable_params / total_params) * 100
            print(f"Trainable Percentage: {trainable_percent:.1f}%")
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'total_params_formatted': format_number(total_params),
            'trainable_params_formatted': format_number(trainable_params)
        }
        
    except Exception as e:
        print(f"âš ï¸ Model complexity analysis failed: {e}")
        return None

def train_fold(config, train_loader, val_loader, fold_idx, k_folds, work_dir):
    """è®­ç»ƒå•ä¸ªæŠ˜"""
    print(f"\n{'='*60}")
    print(f"Training Fold {fold_idx + 1}/{k_folds}")
    print(f"{'='*60}")
    
    # åˆ›å»ºå½“å‰æŠ˜çš„ç›®å½•
    fold_dir = os.path.join(work_dir, f'fold_{fold_idx + 1}')
    os.makedirs(fold_dir, exist_ok=True)
    
    log_dir = os.path.join(fold_dir, 'log')
    checkpoint_dir = os.path.join(fold_dir, 'checkpoints')
    outputs_dir = os.path.join(fold_dir, 'outputs')
    
    for dir_path in [log_dir, checkpoint_dir, outputs_dir]:
        os.makedirs(dir_path, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = get_logger(f'fold_{fold_idx + 1}', log_dir)
    
    # åˆå§‹åŒ–æ¨¡å‹
    print('#----------Initializing Model----------#')
    print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹ç±»å‹: {config.model_type.upper()}")
    
    # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹
    model, model_type_display = create_model_by_type(config.model_type, config)
    
    # å°†æ¨¡å‹ç§»åˆ°GPU
    model = model.cuda()
    
    # è®¡ç®—æ¨¡å‹å¤æ‚åº¦
    complexity_info = calculate_model_complexity(
        model, 
        input_channels=config.input_channels,
        image_size=256  # å‡è®¾å›¾åƒå¤§å°ä¸º256x256
    )
    
    # ä½¿ç”¨DataParallel
    model = torch.nn.DataParallel(model, device_ids=[0], output_device=0)
    
    # æŸå¤±å‡½æ•°
    criterion = BceDiceLoss().cuda()
    
    # ä¼˜åŒ–å™¨
    optimizer = get_optimizer(config, model)
    scheduler = get_scheduler(config, optimizer)
    
    # è®­ç»ƒå¾ªç¯
    best_val_dice = 0.0  # ã€ä¿®æ”¹ã€‘åŸºäºDiceåˆ†æ•°ä¿å­˜æœ€ä½³æ¨¡å‹
    best_val_loss = float('inf')
    best_epoch = 0
    
    for epoch in range(1, config.epochs + 1):
        print(f"\nEpoch {epoch}/{config.epochs}")
        
        # è®­ç»ƒ
        train_loss = train_one_epoch(
            train_loader, model, criterion, optimizer, scheduler,
            epoch, logger, config
        )
        
        # éªŒè¯
        val_loss, val_dice = val_one_epoch(  # ã€ä¿®æ”¹ã€‘æ¥æ”¶diceåˆ†æ•°
            val_loader, model, criterion, epoch, logger, config
        )
        
        if hasattr(config, 'enable_fusion') and config.enable_fusion and USE_ENHANCED_MODEL and epoch % 10 == 0:
            try:
                if hasattr(model.module, 'analyze_fusion'):
                    analysis = model.module.analyze_fusion()
                    if analysis and analysis.get("status") == "success":
                        print(f"\nğŸ” Fold {fold_idx + 1} - Fusion Analysis Epoch {epoch}:")
                        weights = analysis["modal_weights"]
                        if 'T1_mean' in weights:
                            print(f"  T1 weight: {weights['T1_mean']:.3f} Â± {weights['T1_std']:.3f}")
                        if 'SER_mean' in weights:
                            print(f"  SER weight: {weights['SER_mean']:.3f} Â± {weights['SER_std']:.3f}")
                        if 'PE_mean' in weights:
                            print(f"  PE weight: {weights['PE_mean']:.3f} Â± {weights['PE_std']:.3f}")
            except Exception as e:
                print(f"âš ï¸ Fusion analysis failed: {e}")
        
        # ã€ä¿®æ”¹ã€‘ä¿å­˜æœ€ä½³æ¨¡å‹ - åŸºäºDiceåˆ†æ•°
        if val_dice > best_val_dice:
            best_val_dice = val_dice
            best_val_loss = val_loss
            best_epoch = epoch
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(checkpoint_dir, f'best-epoch{epoch}-dice{val_dice:.4f}.pth')  
            
            # æ¸…ç†state_dict - ä¸train_mama_mia_ultralight.pyä¿æŒä¸€è‡´
            def clean_state_dict(state_dict):
                """æ¸…ç†state_dictï¼Œç§»é™¤thopæ·»åŠ çš„é¢å¤–å‚æ•°"""
                cleaned_state_dict = {}
                removed_keys = []
                for key, value in state_dict.items():
                    if 'total_ops' not in key and 'total_params' not in key:
                        cleaned_state_dict[key] = value
                    else:
                        removed_keys.append(key)
                
                if removed_keys:
                    print(f"Cleaned {len(removed_keys)} extra parameters from state_dict")
                return cleaned_state_dict
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': clean_state_dict(model.module.state_dict()),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_dice': val_dice, 
                'train_loss': train_loss,
                'model_type': config.model_type,
                'model_type_display': model_type_display,  
                'complexity_info': complexity_info
            }, model_path)
            
            # åˆ é™¤æ—§çš„bestæ¨¡å‹
            old_models = [f for f in os.listdir(checkpoint_dir) if f.startswith('best-epoch') and f != f'best-epoch{epoch}-dice{val_dice:.4f}.pth']
            for old_model in old_models:
                try:
                    os.remove(os.path.join(checkpoint_dir, old_model))
                except:
                    pass
            
            print(f"âœ… Saved best model for fold {fold_idx + 1} at epoch {epoch} (val_dice: {val_dice:.4f}, val_loss: {val_loss:.4f})")
    
    print(f"\nğŸ¯ Fold {fold_idx + 1} completed. Best epoch: {best_epoch}, Best val_dice: {best_val_dice:.4f}, Best val_loss: {best_val_loss:.4f}")
    
    return {
        'fold_idx': fold_idx + 1,
        'best_epoch': best_epoch,
        'best_val_dice': best_val_dice,  
        'best_val_loss': best_val_loss,
        'fold_dir': fold_dir,
        'best_model_path': os.path.join(checkpoint_dir, f'best-epoch{best_epoch}-dice{best_val_dice:.4f}.pth'),  
        'model_type': model_type_display,
        'complexity_info': complexity_info
    }


def evaluate_fold(fold_info, test_loader, config, dataset_name):
    """è¯„ä¼°å•ä¸ªæŠ˜çš„æ¨¡å‹"""
    print(f"\nEvaluating Fold {fold_info['fold_idx']} on {dataset_name}...")
    
    # åŠ è½½æ£€æŸ¥ç‚¹ä»¥è·å–æ¨¡å‹ç±»å‹
    checkpoint = torch.load(fold_info['best_model_path'], map_location=torch.device('cpu'))
    
    # è·å–ä¿å­˜çš„æ¨¡å‹ç±»å‹
    saved_model_type = checkpoint.get('model_type', 'ultralight')
    
    # åˆ›å»ºç›¸åŒç±»å‹çš„æ¨¡å‹ - ä½¿ç”¨create_model_by_typeç¡®ä¿æ­£ç¡®é…ç½®
    model, _ = create_model_by_type(saved_model_type, config)
    model = model.cuda()
    model = torch.nn.DataParallel(model, device_ids=[0], output_device=0)
    
    # å¤„ç†çŠ¶æ€å­—å…¸ - è¿‡æ»¤æ‰thopæ·»åŠ çš„å‚æ•°
    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint
    
    # è¿‡æ»¤æ‰ä¸å¿…è¦çš„é”®
    filtered_state_dict = {}
    for key, value in state_dict.items():
        if 'total_ops' not in key and 'total_params' not in key:
            filtered_state_dict[key] = value
    
    model.module.load_state_dict(filtered_state_dict, strict=True)
    model.eval()
    
    # åˆ›å»ºä¸´æ—¶logger
    class DummyLogger:
        def info(self, msg):
            print(f"[LOG] {msg}")
    
    dummy_logger = DummyLogger()
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_loss = test_one_epoch(
        test_loader, model, BceDiceLoss().cuda(), 
        dummy_logger, config, test_data_name=f"fold_{fold_info['fold_idx']}_{dataset_name}"
    )
    
    return test_loss


def main():
    args = parse_args()
    
    print("="*60)
    print("MAMA-MIA äº”æŠ˜äº¤å‰éªŒè¯")
    print("="*60)
    print(f"å®éªŒåç§°: {args.name}")
    print(f"æ•°æ®é›†: {args.datasets}")
    print(f"æŠ˜æ•°: {args.k_folds}")
    print(f"éšæœºç§å­: {args.seed}")
    print(f"å¤šæ¨¡æ€: {args.multimodal}")
    print(f"æ¨¡å‹ç±»å‹: {args.model.upper()}")
    
    if args.multimodal and args.enable_fusion:
        print("ğŸ¯ Dynamic Modal Fusion: âœ… ENABLED")
        print(f"   - Test weight method: {args.test_weight_method}")
        print(f"   - Verbose mode: {'âœ… ON' if args.fusion_verbose else 'âŒ OFF'}")
    elif args.multimodal:
        print("ğŸ¯ Dynamic Modal Fusion: âŒ DISABLED")

    
    print(f"è®­ç»ƒepoch: {args.epochs}")
    if args.cross_dataset_test and args.test_datasets:
        print(f"è·¨æ•°æ®é›†æµ‹è¯•: {args.test_datasets}")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºé…ç½®
    config = MamaMiaConfig(
        model_type=args.model,  # è¿™é‡Œä¼ é€’ args.model
        multimodal=args.multimodal,
        datasets_list=args.datasets,
        input_channels=args.input_channels,
        num_workers=args.num_workers,
        seed=args.seed,
        batch_size=args.batch_size,
        epochs=args.epochs,
        lr=args.lr,
        opt=args.opt,
        weight_decay=args.weight_decay,
        sch=args.sch,
        T_max=args.T_max,
        threshold=0.5
    )
    
    # åŒé‡ç¡®ä¿ model_type è¢«æ­£ç¡®è®¾ç½®
    config.model_type = args.model
    print(f"é…ç½®ä¸­çš„æ¨¡å‹ç±»å‹: {config.model_type.upper()}")
    
    # ç¡®ä¿è¾“å…¥é€šé“æ­£ç¡®
    if args.multimodal:
        config.input_channels = 3
        print(f"å¤šæ¨¡æ€è¾“å…¥ï¼Œè®¾ç½® input_channels=3")
    else:
        print(f"å•æ¨¡æ€è¾“å…¥ï¼Œä½¿ç”¨ input_channels={config.input_channels}")
    
    # æ›´æ–°é…ç½®
    config.use_augmentation = args.use_augmentation
    config.balanced_sampling = args.balanced_sampling
    
    config.enable_fusion = args.enable_fusion
    config.fusion_verbose = args.fusion_verbose
    config.test_weight_method = args.test_weight_method
    
    # è®¾ç½®å·¥ä½œç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    config.work_dir = f'results/IJCAI_experiments/{args.name}_{timestamp}'
    os.makedirs(config.work_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_dict = {k: v for k, v in vars(args).items()}
    config_path = os.path.join(config.work_dir, 'config.json')
    with open(config_path, 'w') as f:
        json.dump(config_dict, f, indent=2)
    print(f"\né…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    print("\n#----------Loading Complete Dataset for Cross-Validation----------#")
    
    # é…ç½®æ•°æ®å¢å¼º
    transform = None
    if args.use_augmentation and args.multimodal:
        transform = MAMAMIAMultiModalAugmentation(p=0.5)
        print("å¯ç”¨å¤šæ¨¡æ€æ•°æ®å¢å¼º")
    
    # åˆ›å»ºä¸“é—¨ç”¨äºäº¤å‰éªŒè¯çš„æ•°æ®é›†
    print(f"\nåˆ›å»ºäº¤å‰éªŒè¯æ•°æ®é›†...")
    
    full_dataset = MAMAMIADatasetCV(
        data_dir=config.data_dir,
        seg_dir=config.seg_dir,
        datasets=config.datasets_list,
        mode='train',
        input_channels=config.input_channels,
        multimodal=config.multimodal,
        ser_dir=config.ser_dir,
        pe_dir=config.pe_dir,
        transform=transform,
        seed=config.seed,
        balanced_sampling=args.balanced_sampling,
        cross_dataset_test=False
    )
    
    print(f"\nå®Œæ•´æ•°æ®é›†ä¿¡æ¯:")
    print(f"  åˆ‡ç‰‡æ€»æ•°: {len(full_dataset)}")
    print(f"  æ‚£è€…æ•°é‡: {len(full_dataset.original_dataset.patient_data)}")
    
    # åˆ›å»ºäº”æŠ˜åˆ†å‰²å™¨
    print("\n#----------Creating K-Fold Splits----------#")
    start_time = time.time()
    fold_splitter = KFoldSplitter(full_dataset, n_splits=args.k_folds, seed=args.seed)
    end_time = time.time()
    print(f"åˆ›å»ºäº”æŠ˜åˆ’åˆ†è€—æ—¶: {end_time - start_time:.2f} ç§’")
    
    # å­˜å‚¨ç»“æœ
    fold_results = []

    if args.fold_indices is None:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œè®­ç»ƒæ‰€æœ‰æŠ˜
        folds_to_train = list(range(args.k_folds))
    else:
        # è¿‡æ»¤æœ‰æ•ˆçš„æŠ˜æ•°ç´¢å¼•
        folds_to_train = []
        for idx in args.fold_indices:
            if 0 <= idx < args.k_folds:
                folds_to_train.append(idx)
            else:
                print(f"è­¦å‘Š: æŠ˜ç´¢å¼• {idx} è¶…å‡ºèŒƒå›´ (0-{args.k_folds-1})ï¼Œå°†è¢«å¿½ç•¥")
    
    if not folds_to_train:
        print("é”™è¯¯: æ²¡æœ‰æœ‰æ•ˆçš„æŠ˜æ•°ç´¢å¼•å¯è®­ç»ƒï¼")
        return
    
    print(f"å°†è®­ç»ƒä»¥ä¸‹æŠ˜: {folds_to_train}")
    
    # è¿›è¡Œäº”æŠ˜äº¤å‰éªŒè¯ï¼ˆä»…è®­ç»ƒæŒ‡å®šçš„æŠ˜ï¼‰
    for fold_idx in folds_to_train:
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å¤„ç†ç¬¬ {fold_idx + 1}/{args.k_folds} æŠ˜")
        print(f"{'='*60}")
        
        # åˆ›å»ºå½“å‰æŠ˜çš„æ•°æ®åŠ è½½å™¨
        train_loader, val_loader = create_cross_validation_datasets(
            config, fold_splitter, fold_idx
        )
        
        # æ£€æŸ¥æ•°æ®åŠ è½½å™¨æ˜¯å¦ä¸ºç©º
        if train_loader is None or val_loader is None:
            print(f"è·³è¿‡æŠ˜ {fold_idx}ï¼Œå› ä¸ºæ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥")
            continue
        
        # è®­ç»ƒå½“å‰æŠ˜
        fold_info = train_fold(
            config, train_loader, val_loader, 
            fold_idx, args.k_folds, config.work_dir
        )
        
        # ä¿å­˜ç»“æœ
        fold_results.append(fold_info)
    
    if not fold_results:
        print("é”™è¯¯: æ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æŠ˜!")
        return
    
    # å¦‚æœæœ‰æµ‹è¯•æ•°æ®é›†ï¼Œè¿›è¡Œæµ‹è¯•
    if args.cross_dataset_test and args.test_datasets:
        print(f"\n{'='*60}")
        print(f"è¿›è¡Œè·¨æ•°æ®é›†æµ‹è¯•")
        print(f"{'='*60}")
        
        # åˆ›å»ºæµ‹è¯•é…ç½®
        test_config = MamaMiaConfig(
            multimodal=args.multimodal,
            datasets_list=args.test_datasets,
            input_channels=args.input_channels,
            cross_dataset_test=True,
            num_workers=args.num_workers,
            enable_fusion=args.enable_fusion,
            fusion_verbose=args.fusion_verbose,
            test_weight_method=args.test_weight_method
        )
        
        # ç¡®ä¿æµ‹è¯•é…ç½®ä¹Ÿæœ‰æ­£ç¡®çš„æ¨¡å‹ç±»å‹
        test_config.model_type = config.model_type
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨
        test_data_loader = MAMAMIADataLoader(test_config)
        test_loader = test_data_loader.get_test_loader()
        
        dataset_name = '_'.join(args.test_datasets)
        
        # è¯„ä¼°æ¯ä¸ªæŠ˜çš„æ¨¡å‹
        for fold_info in fold_results:
            try:
                test_loss = evaluate_fold(fold_info, test_loader, config, dataset_name)
                fold_info[f'test_loss_{dataset_name}'] = test_loss
                print(f"æŠ˜ {fold_info['fold_idx']} åœ¨ {dataset_name} ä¸Šçš„æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            except Exception as e:
                print(f"è¯„ä¼°æŠ˜ {fold_info['fold_idx']} æ—¶å‡ºé”™: {e}")
                fold_info[f'test_loss_{dataset_name}'] = None
    
    # ä¿å­˜äº¤å‰éªŒè¯ç»“æœ
    print("\n#----------Saving Cross-Validation Results----------#")
    
    # å‡†å¤‡ç»“æœæ•°æ®
    results_data = []
    for r in fold_results:
        result_row = {
            'fold': r['fold_idx'],
            'best_epoch': r['best_epoch'],
            'best_val_dice': r['best_val_dice'],  # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºbest_val_dice
            'best_val_loss': r['best_val_loss'],
            'model_path': r['best_model_path'],
            'model_type': r.get('model_type', 'Unknown'),  # æ·»åŠ æ¨¡å‹ç±»å‹
        }
        
        # æ·»åŠ æ¨¡å‹å¤æ‚åº¦ä¿¡æ¯
        if r.get('complexity_info'):
            result_row['total_params'] = r['complexity_info'].get('total_params', 'N/A')
            result_row['trainable_params'] = r['complexity_info'].get('trainable_params', 'N/A')
            result_row['total_params_formatted'] = r['complexity_info'].get('total_params_formatted', 'N/A')
            result_row['trainable_params_formatted'] = r['complexity_info'].get('trainable_params_formatted', 'N/A')
        
        # æ·»åŠ æµ‹è¯•ç»“æœ
        for key in r.keys():
            if key.startswith('test_loss_'):
                result_row[key] = r[key]
        
        results_data.append(result_row)
    
    # è½¬æ¢ä¸ºDataFrame
    results_df = pd.DataFrame(results_data)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'mean_val_dice': results_df['best_val_dice'].mean(),  # ã€ä¿®æ”¹ã€‘ç»Ÿè®¡Dice
        'std_val_dice': results_df['best_val_dice'].std(),
        'min_val_dice': results_df['best_val_dice'].min(),
        'max_val_dice': results_df['best_val_dice'].max(),
        'mean_val_loss': results_df['best_val_loss'].mean(),
        'std_val_loss': results_df['best_val_loss'].std(),
        'model_type': args.model.upper(),  # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä¸­çš„æ¨¡å‹ç±»å‹
        'enable_fusion': args.enable_fusion if args.multimodal else False
    }
    
    # æ·»åŠ æ¨¡å‹å¤æ‚åº¦ç»Ÿè®¡
    if 'total_params' in results_df.columns:
        try:
            # åªå¤„ç†æ•°å€¼å‹å‚æ•°
            total_params_numeric = pd.to_numeric(results_df['total_params'], errors='coerce').dropna()
            if len(total_params_numeric) > 0:
                stats['mean_total_params'] = total_params_numeric.mean()
                stats['std_total_params'] = total_params_numeric.std()
        except:
            pass
    
    # æ·»åŠ æµ‹è¯•ç»Ÿè®¡
    test_columns = [col for col in results_df.columns if col.startswith('test_loss_')]
    for test_col in test_columns:
        dataset_name = test_col.replace('test_loss_', '')
        test_values = results_df[test_col].dropna()
        if len(test_values) > 0:
            stats.update({
                f'mean_test_loss_{dataset_name}': test_values.mean(),
                f'std_test_loss_{dataset_name}': test_values.std(),
                f'min_test_loss_{dataset_name}': test_values.min(),
                f'max_test_loss_{dataset_name}': test_values.max(),
            })
    
    # ä¿å­˜ç»“æœ
    results_path = os.path.join(config.work_dir, 'cv_results.csv')
    stats_path = os.path.join(config.work_dir, 'cv_statistics.json')
    
    results_df.to_csv(results_path, index=False)
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ äº”æŠ˜äº¤å‰éªŒè¯å®Œæˆ!")
    print(f"{'='*60}")
    print(f"éªŒè¯Diceç»Ÿè®¡:")  # ã€ä¿®æ”¹ã€‘æ˜¾ç¤ºDiceç»Ÿè®¡
    print(f"  å¹³å‡: {stats['mean_val_dice']:.4f} Â± {stats['std_val_dice']:.4f}")
    print(f"  æœ€å°: {stats['min_val_dice']:.4f}")
    print(f"  æœ€å¤§: {stats['max_val_dice']:.4f}")
    print(f"\néªŒè¯æŸå¤±ç»Ÿè®¡:")
    print(f"  å¹³å‡: {stats['mean_val_loss']:.4f} Â± {stats['std_val_loss']:.4f}")
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  æ¨¡å‹ç±»å‹: {stats['model_type']}")
    if args.multimodal:
        print(f"  åŠ¨æ€èåˆ: {'âœ… ENABLED' if stats['enable_fusion'] else 'âŒ DISABLED'}")
    
    # æ‰“å°æ¨¡å‹å¤æ‚åº¦ä¿¡æ¯
    if 'mean_total_params' in stats:
        print(f"\næ¨¡å‹å¤æ‚åº¦:")
        print(f"  å¹³å‡å‚æ•°é‡: {stats['mean_total_params']:,.0f} Â± {stats['std_total_params']:,.0f}")
    
    # æ‰“å°æµ‹è¯•ç»“æœ
    for test_col in test_columns:
        dataset_name = test_col.replace('test_loss_', '')
        if f'mean_test_loss_{dataset_name}' in stats:
            print(f"\nåœ¨ {dataset_name} ä¸Šçš„æµ‹è¯•ç»“æœ:")
            print(f"  å¹³å‡: {stats[f'mean_test_loss_{dataset_name}']:.4f} Â± {stats[f'std_test_loss_{dataset_name}']:.4f}")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {config.work_dir}")
    print(f"  - è¯¦ç»†ç»“æœ: {results_path}")
    print(f"  - ç»Ÿè®¡ä¿¡æ¯: {stats_path}")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if len(results_df) > 0:
        best_fold_idx = results_df['best_val_dice'].idxmax()  # ã€ä¿®æ”¹ã€‘åŸºäºDiceé€‰æ‹©æœ€ä½³æ¨¡å‹
        best_fold_info = fold_results[best_fold_idx]
        
        best_model_dest = os.path.join(config.work_dir, 'best_model.pth')
        try:
            shutil.copy(best_fold_info['best_model_path'], best_model_dest)
            print(f"  - æœ€ä½³æ¨¡å‹å·²å¤åˆ¶åˆ°: {best_model_dest} (æ¥è‡ªæŠ˜ {best_fold_idx}, Dice: {best_fold_info['best_val_dice']:.4f})")
        except Exception as e:
            print(f"  - å¤åˆ¶æœ€ä½³æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
    report_path = os.path.join(config.work_dir, 'summary_report.txt')
    with open(report_path, 'w') as f:
        f.write("="*60 + "\n")
        f.write("MAMA-MIA äº”æŠ˜äº¤å‰éªŒè¯æ±‡æ€»æŠ¥å‘Š\n")
        f.write("="*60 + "\n\n")
        f.write(f"å®éªŒåç§°: {args.name}\n")
        f.write(f"æ•°æ®é›†: {args.datasets}\n")
        f.write(f"æŠ˜æ•°: {args.k_folds}\n")
        f.write(f"å®é™…è®­ç»ƒçš„æŠ˜æ•°: {len(folds_to_train)} (ç´¢å¼•: {folds_to_train})\n")
        f.write(f"éšæœºç§å­: {args.seed}\n")
        f.write(f"å¤šæ¨¡æ€: {args.multimodal}\n")
        f.write(f"æ¨¡å‹ç±»å‹: {args.model.upper()}\n")
        if args.multimodal and args.enable_fusion:
            f.write(f"åŠ¨æ€èåˆ: âœ… ENABLED (method: {args.test_weight_method})\n")
        f.write("\n")
        
        f.write("éªŒè¯Diceç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡: {stats['mean_val_dice']:.4f} Â± {stats['std_val_dice']:.4f}\n")
        f.write(f"  æœ€å°: {stats['min_val_dice']:.4f}\n")
        f.write(f"  æœ€å¤§: {stats['max_val_dice']:.4f}\n\n")
        
        f.write("éªŒè¯æŸå¤±ç»Ÿè®¡:\n")
        f.write(f"  å¹³å‡: {stats['mean_val_loss']:.4f} Â± {stats['std_val_loss']:.4f}\n\n")
        
        if 'mean_total_params' in stats:
            f.write("æ¨¡å‹å¤æ‚åº¦ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡å‚æ•°é‡: {stats['mean_total_params']:,.0f} Â± {stats['std_total_params']:,.0f}\n\n")
        
        if test_columns:
            f.write("è·¨æ•°æ®é›†æµ‹è¯•ç»“æœ:\n")
            for test_col in test_columns:
                dataset_name = test_col.replace('test_loss_', '')
                if f'mean_test_loss_{dataset_name}' in stats:
                    f.write(f"  {dataset_name}:\n")
                    f.write(f"    å¹³å‡: {stats[f'mean_test_loss_{dataset_name}']:.4f} Â± {stats[f'std_test_loss_{dataset_name}']:.4f}\n")
        
        if len(results_df) > 0:
            best_fold_idx = results_df['best_val_dice'].idxmax()  # ã€ä¿®æ”¹ã€‘åŸºäºDice
            f.write(f"\næœ€ä½³æ¨¡å‹æ¥è‡ª: æŠ˜ {best_fold_idx}\n")
            f.write(f"æœ€ä½³Diceåˆ†æ•°: {results_df.loc[best_fold_idx, 'best_val_dice']:.4f}\n")
            best_model_type = results_df.loc[best_fold_idx, 'model_type'] if 'model_type' in results_df.columns else args.model.upper()
            f.write(f"æœ€ä½³æ¨¡å‹ç±»å‹: {best_model_type}\n")
    
    print(f"  - æ±‡æ€»æŠ¥å‘Š: {report_path}")


if __name__ == '__main__':

    main()
